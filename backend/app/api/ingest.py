from fastapi import APIRouter, Depends, UploadFile, File, HTTPException
from fastapi.responses import JSONResponse
from sqlalchemy.orm import Session
from app.db import SessionLocal
from app.models import Content, Chunk
from app.adapters import webdav
from pydantic import BaseModel
from typing import List
import logging
import os
import uuid
from pathlib import Path
from app.workers.tasks import ingest_file as ingest_task, generate_embeddings, simple_chunk, classify_content
from app.parsers.document_processor import DocumentProcessor

router = APIRouter()
log = logging.getLogger(__name__)

def get_db():
    db = SessionLocal()
    try: yield db
    finally: db.close()

# simple_chunk å‡½æ•°å·²ç§»åŠ¨åˆ° workers/tasks.py ä¸­

@router.post("/memo")
def ingest_memo(item: dict, db: Session = Depends(get_db)):
    title = item.get("title") or "memo"
    text  = item.get("text") or ""
    meta  = item.get("meta", {})
    
    # ç¡®å®šæ–‡æ¡£ç±»å‹
    detected_type = meta.get("detected_type", "text")
    modality = 'image' if detected_type == 'image' else 'text'
    
    content = Content(
        title=title, 
        text=text, 
        modality=modality,
        meta=meta, 
        source_uri=item.get("source_uri"), 
        created_by="memo.api"
    )
    db.add(content); db.commit(); db.refresh(content)

    # åˆ‡ç‰‡å¹¶å…¥åº“
    seq = 0
    chunk_ids = []
    for t in simple_chunk(text):
        chunk = Chunk(content_id=content.id, seq=seq, text=t, meta={"source_uri": content.source_uri})
        db.add(chunk)
        seq += 1
    db.commit()
    
    # è·å–æ–°åˆ›å»ºçš„ chunk IDs
    db.refresh(content)
    chunk_ids = [str(chunk.id) for chunk in content.chunks]
    
    # å¼‚æ­¥ç”Ÿæˆ embeddings
    if chunk_ids:
        generate_embeddings.delay(chunk_ids)
    
    # ç«‹å³è¿›è¡Œå¿«é€Ÿåˆ†ç±»ï¼ˆå¼‚æ­¥ï¼Œæœ€é«˜ä¼˜å…ˆçº§ï¼‰
    from app.workers.quick_tasks import quick_classify_content
    quick_classify_content.apply_async(
        args=[str(content.id)], 
        queue="quick", 
        priority=10,  # æé«˜ä¼˜å…ˆçº§ç¡®ä¿å…ˆæ‰§è¡Œ
        countdown=1
    )
    
    # æ™ºèƒ½åˆé›†åŒ¹é… - ä¼˜å…ˆåŒæ­¥æ‰§è¡Œä»¥ç¡®ä¿ç«‹å³ç”Ÿæ•ˆ
    try:
        from app.services.collection_matching_service import CollectionMatchingService
        matching_service = CollectionMatchingService(db)
        matched = matching_service.match_document_to_collections(str(content.id))
        log.info(f"Synchronously matched content {content.id} to {len(matched)} collections")
        
        # å¼‚æ­¥ä»»åŠ¡ä½œä¸ºå¤‡ä»½ï¼ˆå¦‚æœåŒæ­¥æ‰§è¡ŒæˆåŠŸï¼Œè¿™ä¸ªä»»åŠ¡ä¼šå¿«é€Ÿè·³è¿‡ï¼‰
        from app.workers.quick_tasks import match_document_to_collections
        match_document_to_collections.apply_async(
            args=[str(content.id)],
            queue="quick",
            priority=8,
            countdown=8   # ä½œä¸ºå¤‡ä»½ï¼Œå»¶è¿Ÿ8ç§’æ‰§è¡Œ
        )
        
    except Exception as e:
        log.error(f"Synchronous collection matching failed for {content.id}: {e}")
        # å¦‚æœåŒæ­¥å¤±è´¥ï¼Œä¾èµ–å¼‚æ­¥ä»»åŠ¡
        try:
            from app.workers.quick_tasks import match_document_to_collections
            match_document_to_collections.apply_async(
                args=[str(content.id)],
                queue="quick",
                priority=9,
                countdown=2  # æ›´çŸ­çš„å»¶è¿Ÿ
            )
            log.info(f"Fallback: Scheduled async collection matching for content {content.id}")
        except Exception as async_e:
            log.error(f"Both sync and async collection matching failed: {async_e}")
    
    # å¼‚æ­¥è¿›è¡Œç²¾ç¡®AIåˆ†ç±»ï¼ˆæœ€åæ‰§è¡Œï¼Œè¦†ç›–å¿«é€Ÿåˆ†ç±»ï¼‰
    classify_content.apply_async(
        args=[str(content.id)], 
        queue="classify", 
        priority=5,
        countdown=6   # å»¶è¿Ÿ6ç§’ï¼Œç¡®ä¿å‰é¢çš„ä»»åŠ¡éƒ½å®Œæˆ
    )
    
    return {"status":"ok","content_id":str(content.id),"chunks":seq}

@router.post("/scan")
def ingest_scan(db: Session = Depends(get_db)):
    """
    æ‰«æ Nextcloud WebDAV çš„ PKB-Inboxï¼Œæ£€æŸ¥å»é‡åå…¥åº“ï¼Œå¹¶æ¸…ç†å·²åˆ é™¤çš„æ–‡ä»¶
    """
    docs = webdav.scan_inbox()
    total_chunks = 0
    all_chunk_ids = []
    new_content_ids = []
    new_files = 0
    skipped_files = 0
    deleted_files = 0
    
    # è·å–å½“å‰æ‰«æåˆ°çš„æ‰€æœ‰æ–‡ä»¶çš„ source_uri
    current_source_uris = {d["source_uri"] for d in docs}
    
    # æŸ¥æ‰¾æ•°æ®åº“ä¸­æ‰€æœ‰æ¥è‡ª webdav çš„å†…å®¹
    existing_contents = db.query(Content).filter(
        Content.source_uri.like("nextcloud://%")
    ).all()
    
    # è¯†åˆ«éœ€è¦åˆ é™¤çš„å†…å®¹ï¼ˆåœ¨æ•°æ®åº“ä¸­ä½†ä¸åœ¨å½“å‰æ‰«æç»“æœä¸­ï¼‰
    for existing_content in existing_contents:
        if existing_content.source_uri not in current_source_uris:
            log.info(f"File deleted from Nextcloud, removing from database: {existing_content.title}")
            
            # åˆ é™¤ç›¸å…³çš„åˆ†ç±»å…³è”
            from app.models import ContentCategory
            db.query(ContentCategory).filter(
                ContentCategory.content_id == existing_content.id
            ).delete()
            
            # åˆ é™¤ç›¸å…³çš„ chunksï¼ˆä¼šè‡ªåŠ¨åˆ é™¤ embeddingsï¼‰
            db.query(Chunk).filter(
                Chunk.content_id == existing_content.id
            ).delete()
            
            # åˆ é™¤å†…å®¹è®°å½•
            db.delete(existing_content)
            deleted_files += 1
    
    db.commit()
    
    # å¤„ç†æ–°æ–‡ä»¶
    for d in docs:
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²ç»å­˜åœ¨ï¼ˆåŸºäº source_uriï¼‰
        existing_content = db.query(Content).filter(
            Content.source_uri == d["source_uri"]
        ).first()
        
        if existing_content:
            log.info(f"File already exists, skipping: {d['title']}")
            skipped_files += 1
            continue
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ç”¨æˆ·ä¸»åŠ¨åˆ é™¤çš„æ–‡ä»¶ï¼ˆé€šè¿‡åˆ é™¤è®°å½•æ£€æŸ¥ï¼‰
        from app.models import OpsLog
        from sqlalchemy import text
        
        # ä½¿ç”¨åŸç”Ÿ SQL æŸ¥è¯¢æ¥æ£€æŸ¥ JSON å­—æ®µ
        delete_record = db.query(OpsLog).filter(
            OpsLog.op_type == "user_delete",
            text("payload->>'source_uri' = :source_uri"),
            OpsLog.status == "completed"
        ).params(source_uri=d["source_uri"]).first()
        
        if delete_record:
            log.info(f"File was user-deleted, skipping re-creation: {d['title']}")
            skipped_files += 1
            continue
        
        # ç¡®å®šæ–‡æ¡£ç±»å‹
        detected_type = d.get("metadata", {}).get("detected_type", "text")
        modality = 'image' if detected_type == 'image' else 'text'
        
        # åˆ›å»ºæ–°çš„å†…å®¹è®°å½•
        content = Content(
            title=d["title"], 
            text=d["text"],
            modality=modality,
            meta=d.get("metadata", {}), 
            source_uri=d["source_uri"], 
            created_by="webdav.scan"
        )
        db.add(content)
        db.commit()
        db.refresh(content)
        new_files += 1
        new_content_ids.append(str(content.id))

        # æ–‡æœ¬åˆ†å—
        seq = 0
        for t in simple_chunk(d["text"]):
            chunk = Chunk(
                content_id=content.id, 
                seq=seq, 
                text=t, 
                meta={"source_uri": content.source_uri}
            )
            db.add(chunk)
            seq += 1
        db.commit()
        
        # æ”¶é›†æ–°åˆ›å»ºçš„ chunk IDs
        db.refresh(content)
        chunk_ids = [str(chunk.id) for chunk in content.chunks]
        all_chunk_ids.extend(chunk_ids)
        total_chunks += seq
        
        log.info(f"Processed new file: {d['title']} ({seq} chunks)")
    
    # å¼‚æ­¥ç”Ÿæˆ embeddingsï¼ˆä»…ä¸ºæ–°æ–‡æ¡£ï¼‰
    if all_chunk_ids:
        generate_embeddings.delay(all_chunk_ids)
    
    # å¤„ç†æ–°æ–‡æ¡£çš„åˆ†ç±»å’Œå›¾ç‰‡å¤„ç†
    if new_content_ids:
        # åˆ†ç¦»å›¾ç‰‡å’Œéå›¾ç‰‡å†…å®¹
        image_content_ids = []
        text_content_ids = []
        
        for content_id in new_content_ids:
            content = db.query(Content).filter(Content.id == content_id).first()
            if content and content.modality == 'image' and content.meta.get("processing_status") == "pending":
                image_content_ids.append(content_id)
            else:
                text_content_ids.append(content_id)
        
        # ç«‹å³æ‰¹é‡å¿«é€Ÿåˆ†ç±»éå›¾ç‰‡æ–‡æ¡£
        if text_content_ids:
            from app.workers.quick_tasks import batch_quick_classify
            batch_quick_classify.apply_async(
                args=[text_content_ids], 
                queue="quick", 
                priority=9
            )
            
            # å»¶è¿Ÿè¿›è¡Œç²¾ç¡®AIåˆ†ç±»
            from app.workers.tasks import batch_classify_contents
            batch_classify_contents.apply_async(
                args=[text_content_ids, True],  # force_reclassify=True
                queue="classify", 
                priority=5,
                countdown=30
            )
        
        # å¼‚æ­¥å¤„ç†å›¾ç‰‡æ–‡ä»¶
        if image_content_ids:
            from app.workers.tasks import process_image_content
            for content_id in image_content_ids:
                process_image_content.apply_async(
                    args=[content_id],
                    queue="heavy",
                    priority=3,
                    countdown=5  # 5ç§’åå¼€å§‹å¤„ç†
                )
    
    return {
        "status": "ok",
        "files_scanned": len(docs),
        "new_files": new_files,
        "skipped_files": skipped_files,
        "deleted_files": deleted_files,
        "new_chunks": total_chunks
    }

class IngestRequest(BaseModel):
    path: str

@router.post("/upload")
async def upload_file(file: UploadFile = File(...), db: Session = Depends(get_db)):
    """å•æ–‡ä»¶ä¸Šä¼ æ¥å£"""
    # MVPé™åˆ¶ï¼šéªŒè¯å•ä¸ªæ–‡ä»¶å¤§å°
    max_single_size = 20 * 1024 * 1024  # 20MB
    if file.size and file.size > max_single_size:
        raise HTTPException(
            status_code=400,
            detail=f"MVPé˜¶æ®µå•ä¸ªæ–‡ä»¶ä¸èƒ½è¶…è¿‡20MBï¼Œå½“å‰æ–‡ä»¶ {file.filename} å¤§å°ä¸º {file.size / 1024 / 1024:.1f}MB"
        )
    
    return await _process_single_file(file, db)

@router.post("/upload-multiple")
async def upload_multiple_files(files: List[UploadFile] = File(...), db: Session = Depends(get_db)):
    """
    å¤šæ–‡ä»¶æ‰¹é‡ä¸Šä¼ æ¥å£
    """
    try:
        results = []
        
        # MVPé™åˆ¶ï¼šéªŒè¯æ–‡ä»¶æ•°é‡é™åˆ¶
        if len(files) > 5:  # MVPé˜¶æ®µé™åˆ¶æœ€å¤š5ä¸ªæ–‡ä»¶
            raise HTTPException(
                status_code=400,
                detail="MVPé˜¶æ®µä¸€æ¬¡æœ€å¤šåªèƒ½ä¸Šä¼ 5ä¸ªæ–‡ä»¶"
            )
        
        # MVPé™åˆ¶ï¼šéªŒè¯å•ä¸ªæ–‡ä»¶å¤§å°
        max_single_size = 20 * 1024 * 1024  # 20MB
        oversized_files = [f for f in files if f.size and f.size > max_single_size]
        if oversized_files:
            oversized_names = [f"{f.filename}({f.size / 1024 / 1024:.1f}MB)" for f in oversized_files]
            raise HTTPException(
                status_code=400,
                detail=f"MVPé˜¶æ®µå•ä¸ªæ–‡ä»¶ä¸èƒ½è¶…è¿‡20MBï¼Œä»¥ä¸‹æ–‡ä»¶è¶…é™ï¼š{', '.join(oversized_names)}"
            )
        
        # MVPé™åˆ¶ï¼šéªŒè¯æ€»æ–‡ä»¶å¤§å°
        total_size = sum(file.size for file in files if file.size)
        max_total_size = 100 * 1024 * 1024  # 100MB (5ä¸ªæ–‡ä»¶Ã—20MB)
        if total_size > max_total_size:
            raise HTTPException(
                status_code=400,
                detail=f"MVPé˜¶æ®µæ‰¹é‡ä¸Šä¼ æ€»å¤§å°ä¸èƒ½è¶…è¿‡100MBï¼Œå½“å‰ï¼š{total_size / 1024 / 1024:.1f}MB"
            )
        
        # å¤„ç†æ¯ä¸ªæ–‡ä»¶
        for i, file in enumerate(files):
            try:
                result = await _process_single_file(file, db)
                results.append({
                    "index": i,
                    "filename": file.filename,
                    "status": "success",
                    "result": result
                })
            except Exception as e:
                log.error(f"Failed to process file {file.filename}: {e}")
                results.append({
                    "index": i,
                    "filename": file.filename,
                    "status": "error",
                    "error": str(e)
                })
        
        # ç»Ÿè®¡ç»“æœ
        success_count = len([r for r in results if r["status"] == "success"])
        error_count = len([r for r in results if r["status"] == "error"])
        
        return {
            "status": "completed",
            "total_files": len(files),
            "success_count": success_count,
            "error_count": error_count,
            "results": results,
            "message": f"æ‰¹é‡ä¸Šä¼ å®Œæˆï¼š{success_count}ä¸ªæˆåŠŸï¼Œ{error_count}ä¸ªå¤±è´¥"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        log.error(f"Batch upload error: {e}")
        raise HTTPException(status_code=500, detail=f"æ‰¹é‡ä¸Šä¼ å¤±è´¥: {str(e)}")

async def _process_single_file(file: UploadFile, db: Session):
    """
    WebUIæ–‡ä»¶ä¸Šä¼ æ¥å£ - ç«‹å³å…¥åº“ï¼Œå¼‚æ­¥å¤„ç†
    """
    try:
        # 1. éªŒè¯æ–‡ä»¶ç±»å‹
        processor = DocumentProcessor()
        if not processor.is_supported(file.filename):
            raise HTTPException(
                status_code=400, 
                detail=f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file.filename}"
            )
        
        # 2. ä¿å­˜æ–‡ä»¶åˆ°æŒä¹…åŒ–ç›®å½•
        upload_dir = Path("/app/uploads")
        upload_dir.mkdir(parents=True, exist_ok=True)
        
        # ä½¿ç”¨åŸå§‹æ–‡ä»¶åï¼Œå¤„ç†é‡åå†²çª
        def get_unique_filename(upload_dir: Path, original_filename: str) -> str:
            """ç”Ÿæˆå”¯ä¸€æ–‡ä»¶åï¼Œé‡åæ—¶æ·»åŠ æ—¶é—´æˆ³"""
            base_path = upload_dir / original_filename
            
            if not base_path.exists():
                return original_filename
            
            # ç”Ÿæˆæ—¶é—´æˆ³
            from datetime import datetime
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            name_part = Path(original_filename).stem
            extension = Path(original_filename).suffix
            
            return f"{name_part}_{timestamp}{extension}"
        
        actual_filename = get_unique_filename(upload_dir, file.filename)
        temp_file_path = upload_dir / actual_filename
        
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
        with open(temp_file_path, "wb") as buffer:
            content = await file.read()
            buffer.write(content)
        
        # 3. å¿«é€Ÿæ£€æµ‹æ–‡ä»¶ç±»å‹å’ŒåŸºæœ¬ä¿¡æ¯ï¼ˆåŒæ­¥ï¼Œå¿«é€Ÿï¼‰
        file_size = len(content)
        file_extension = Path(file.filename).suffix.lower()
        
        # åˆ¤æ–­æ–‡ä»¶ç±»å‹
        is_image = file_extension in ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp']
        is_large_file = file_size > 10 * 1024 * 1024  # 10MB
        
        # åŸºæœ¬å…ƒæ•°æ®ï¼ˆä¸éœ€è¦è§£ææ–‡ä»¶å†…å®¹ï¼‰
        basic_metadata = {
            "source_type": "webui",
            "file_size": file_size,
            "file_extension": file_extension,
            "is_image": is_image,
            "is_large_file": is_large_file,
            "upload_timestamp": str(uuid.uuid4())
        }
        
        log.info(f"ğŸ“ File uploaded: {file.filename} ({file_size/1024/1024:.1f}MB, {'image' if is_image else 'document'})")
        
        # 4. ç«‹å³åˆ›å»ºContentè®°å½•ï¼ˆä¸Šä¼ å®Œæˆï¼Œç­‰å¾…è§£æï¼‰
        content_record = Content(
            title=file.filename,  # å§‹ç»ˆä¿å­˜ç”¨æˆ·åŸå§‹æ–‡ä»¶å
            text="",  # æ–‡æœ¬å†…å®¹å°†åœ¨å¼‚æ­¥è§£æåå¡«å……
            modality='image' if is_image else 'text',
            meta={
                **basic_metadata,
                "classification_status": "pending",     # åˆ†ç±»çŠ¶æ€
                "show_classification": True,            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šç«‹å³å…è®¸æ˜¾ç¤ºçŠ¶æ€
                "processing_status": "uploaded",        # ä¸Šä¼ å®Œæˆï¼Œç­‰å¾…è§£æ
                "parsing_status": "pending",            # è§£æçŠ¶æ€
                "file_path": str(temp_file_path),       # ä¿å­˜å®é™…æ–‡ä»¶è·¯å¾„
                "original_filename": file.filename,     # ç”¨æˆ·ä¸Šä¼ çš„åŸå§‹æ–‡ä»¶å
                "stored_filename": actual_filename,     # å®é™…å­˜å‚¨çš„æ–‡ä»¶å
            },
            source_uri=f"webui://{actual_filename}",  # ä½¿ç”¨å®é™…å­˜å‚¨çš„æ–‡ä»¶å
            created_by="webui.upload"
        )
        
        db.add(content_record)
        db.commit()
        db.refresh(content_record)
        
        # 5. å¼‚æ­¥ä»»åŠ¡è°ƒåº¦ - ä¼˜åŒ–æ—¶åºå’Œä¼˜å…ˆçº§
        content_id = str(content_record.id)
        
        # ç«‹å³è°ƒåº¦æ–‡ä»¶è§£æä»»åŠ¡ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
        from app.workers.tasks import parse_and_chunk_file
        parse_and_chunk_file.apply_async(
            args=[content_id, str(temp_file_path)],
            queue="quick",
            priority=10,  # æœ€é«˜ä¼˜å…ˆçº§
            countdown=0.1  # å‡ ä¹ç«‹å³æ‰§è¡Œ
        )
        
        log.info(f"ğŸš€ Scheduled parsing for content {content_id}: {file.filename}")
        
        # 6. å›¾ç‰‡æ–‡ä»¶ç‰¹æ®Šå¤„ç†
        if is_image:
            # å¼‚æ­¥ç”Ÿæˆç¼©ç•¥å›¾
            from app.workers.tasks import generate_image_thumbnail
            generate_image_thumbnail.apply_async(
                args=[content_id, str(temp_file_path)],
                queue="heavy",
                priority=7,
                countdown=0.5
            )
        
        # 7. åˆ†ç±»ä»»åŠ¡é“¾ - ä¼˜åŒ–æ‰§è¡Œé¡ºåºç¡®ä¿å®Œæ•´åˆ†ç±»
        # å¿«é€Ÿåˆ†ç±»ï¼ˆ2ç§’åæ‰§è¡Œï¼Œç»™è§£æä»»åŠ¡æ—¶é—´ï¼‰
        from app.workers.quick_tasks import quick_classify_content
        quick_classify_content.apply_async(
            args=[content_id],
            queue="quick",
            priority=9,
            countdown=2  # ç­‰å¾…è§£æå®Œæˆ
        )
        
        # AIç²¾ç¡®åˆ†ç±»ï¼ˆ4ç§’åæ‰§è¡Œï¼‰
        classify_content.apply_async(
            args=[content_id],
            queue="classify",
            priority=8,
            countdown=4
        )
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ™ºèƒ½åˆé›†åŒ¹é…åœ¨AIåˆ†ç±»å®Œæˆåæ‰§è¡Œ
        from app.workers.quick_tasks import match_document_to_collections
        match_document_to_collections.apply_async(
            args=[content_id],
            queue="quick",
            priority=7,
            countdown=10  # ç¡®ä¿AIåˆ†ç±»å®Œæˆåå†æ‰§è¡Œï¼ˆ4så¯åŠ¨ + 3sæ‰§è¡Œ + 2sç¼“å†²ï¼‰
        )
        
        # 8. è¿”å›ç»“æœ - ç«‹å³å“åº”
        return {
            "status": "success",
            "content_id": content_id,
            "title": file.filename,
            "processing_status": "uploaded",  # ä¸Šä¼ å®Œæˆ
            "parsing_status": "pending",      # ç­‰å¾…è§£æ
            "file_size": file_size,
            "file_type": "image" if is_image else "document",
            "estimated_processing_time": 3 if is_image else (8 if is_large_file else 5),  # é¢„ä¼°å¤„ç†æ—¶é—´ï¼ˆç§’ï¼‰
            "message": f"æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼{'å›¾ç‰‡' if is_image else 'æ–‡æ¡£'}æ­£åœ¨åå°è§£æå’Œåˆ†ç±»..."
        }
        
    except HTTPException:
        raise
    except Exception as e:
        log.error(f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}")

@router.get("/status/{content_id}")
def get_processing_status(content_id: str, db: Session = Depends(get_db)):
    """
    æŸ¥è¯¢æ–‡ä»¶å¤„ç†çŠ¶æ€å’Œåˆ†ç±»çŠ¶æ€
    """
    try:
        content = db.query(Content).filter(Content.id == content_id).first()
        if not content:
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        
        # è·å–è¯¦ç»†çŠ¶æ€ä¿¡æ¯
        meta = content.meta or {}
        classification_status = meta.get("classification_status", "pending")
        show_classification = meta.get("show_classification", False)
        processing_status = meta.get("processing_status", "unknown")
        parsing_status = meta.get("parsing_status", "pending")
        
        # æ™ºèƒ½çŠ¶æ€åˆ¤æ–­ï¼šå¦‚æœè§£æå®Œæˆä½†åˆ†ç±»æœªå¼€å§‹ï¼Œæ˜¾ç¤ºåˆ†ç±»ä¸­
        if parsing_status == "completed" and classification_status == "pending":
            classification_status = "quick_processing"
        
        # æ£€æŸ¥åˆ†ç±»ç»“æœ
        from app.models import ContentCategory
        categories = db.query(ContentCategory).filter(
            ContentCategory.content_id == content_id
        ).all()
        
        # æ„å»ºè¿”å›ç»“æœ
        result = {
            "content_id": content_id,
            "title": content.title,
            "processing_status": processing_status,
            "parsing_status": parsing_status,
            "classification_status": classification_status,
            "show_classification": show_classification,
            "file_type": meta.get("file_type", "document"),
            "file_size": meta.get("file_size", 0),
            "estimated_time": meta.get("estimated_processing_time", 5),
            "created_at": content.created_at.isoformat() if content.created_at else None
        }
        
        # åªæœ‰åœ¨å…è®¸æ˜¾ç¤ºåˆ†ç±»æ—¶æ‰è¿”å›åˆ†ç±»ä¿¡æ¯
        if show_classification and categories:
            # åˆ†ç¦»ä¸»åˆ†ç±»å’Œæ¬¡è¦åˆ†ç±»
            primary_categories = []
            secondary_categories = []
            
            for cat in categories:
                category_info = {
                    "id": str(cat.category_id),
                    "name": cat.category.name if cat.category else "Unknown",
                    "confidence": cat.confidence,
                    "reasoning": cat.reasoning,
                    "role": cat.role,
                    "source": cat.source
                }
                
                if cat.role == "primary_system":
                    primary_categories.append(category_info)
                elif cat.role == "secondary_system":
                    secondary_categories.append(category_info)
                else:
                    # å…¶ä»–è§’è‰²ï¼ˆå¦‚ç”¨æˆ·è§„åˆ™ï¼‰ä¹ŸåŒ…å«åœ¨å†…
                    primary_categories.append(category_info)
            
            result["categories"] = primary_categories + secondary_categories
            result["primary_categories"] = primary_categories
            result["secondary_categories"] = secondary_categories
        else:
            result["categories"] = []
            result["primary_categories"] = []
            result["secondary_categories"] = []
            result["message"] = "åˆ†ç±»ä¸­..." if classification_status == "pending" else "åˆ†ç±»å¤„ç†ä¸­"
        
        return result
        
    except HTTPException:
        raise
    except Exception as e:
        log.error(f"æŸ¥è¯¢çŠ¶æ€å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æŸ¥è¯¢çŠ¶æ€å¤±è´¥: {str(e)}")

@router.post("/file")
async def ingest_file_endpoint(req: IngestRequest):
    log.info("INGEST schedule: %s", req.path)
    result = ingest_task.apply_async(args=[req.path], queue="ingest")
    payload = {"task_id": str(result.id), "message": f"File {req.path} scheduled for ingest"}
    return JSONResponse(content=payload, status_code=202)

