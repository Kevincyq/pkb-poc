"""
æ™ºèƒ½åˆé›†åŒ¹é…æœåŠ¡
ç”¨äºè‡ªåŠ¨å°†æ–‡æ¡£åŒ¹é…åˆ°ç”¨æˆ·åˆ›å»ºçš„åˆé›†ä¸­
"""

import logging
import re
from typing import Dict, List, Any, Optional
from sqlalchemy.orm import Session
from sqlalchemy import func

from app.models import Collection, Content, ContentCategory, Category

logger = logging.getLogger(__name__)

class CollectionMatchingService:
    """æ™ºèƒ½åˆé›†åŒ¹é…æœåŠ¡"""
    
    def __init__(self, db: Session):
        self.db = db
    
    def generate_auto_match_rules(self, collection_name: str, description: str = None) -> Dict[str, Any]:
        """
        æ ¹æ®åˆé›†åç§°å’Œæè¿°ç”Ÿæˆè‡ªåŠ¨åŒ¹é…è§„åˆ™
        
        Args:
            collection_name: åˆé›†åç§°
            description: åˆé›†æè¿°
            
        Returns:
            åŒ¹é…è§„åˆ™å­—å…¸
        """
        try:
            # åŸºäºåˆé›†åç§°ç”Ÿæˆå…³é”®è¯
            keywords = self._extract_keywords_from_name(collection_name)
            
            # å¦‚æœæœ‰æè¿°ï¼Œä»æè¿°ä¸­æå–æ›´å¤šå…³é”®è¯
            if description:
                desc_keywords = self._extract_keywords_from_description(description)
                keywords.extend(desc_keywords)
            
            # ç”Ÿæˆæ ‡é¢˜åŒ¹é…æ¨¡å¼
            title_patterns = self._generate_title_patterns(collection_name, keywords)
            
            # ç”Ÿæˆå†…å®¹åŒ¹é…æ¨¡å¼
            content_patterns = self._generate_content_patterns(keywords)
            
            rules = {
                "keywords": list(set(keywords)),  # å»é‡
                "title_patterns": title_patterns,
                "content_patterns": content_patterns,
                "auto_match": True,
                "match_threshold": 0.6  # åŒ¹é…é˜ˆå€¼
            }
            
            logger.info(f"Generated auto-match rules for collection '{collection_name}': {rules}")
            return rules
            
        except Exception as e:
            logger.error(f"Error generating auto-match rules: {e}")
            return {"auto_match": False}
    
    def _extract_keywords_from_name(self, name: str) -> List[str]:
        """ä»åˆé›†åç§°ä¸­æå–å…³é”®è¯"""
        keywords = []
        
        # é¢„å®šä¹‰çš„å…³é”®è¯æ˜ å°„ï¼ˆä½œä¸ºç§å­è¯æ±‡ï¼Œå¯æ‰©å±•ï¼‰
        keyword_mapping = {
            "ä¼šè®®çºªè¦": ["ä¼šè®®", "çºªè¦", "meeting", "minutes", "è®®é¢˜", "å†³è®®", "å‚ä¼š", "è®¨è®º", "ä¼šè°ˆ"],
            "æ—…æ¸¸": ["æ—…æ¸¸", "æ—…è¡Œ", "åº¦å‡", "vacation", "travel", "æ™¯ç‚¹", "é£æ™¯", "æµ·è¾¹", "å±±æ°´", "è¿ªæ–¯å°¼", "æ™¯åŒº", "é…’åº—", "å¥—é¤"],
            "é¡¹ç›®æ–‡æ¡£": ["é¡¹ç›®", "project", "è®¡åˆ’", "æ–¹æ¡ˆ", "éœ€æ±‚", "è®¾è®¡"],
            "æŠ€æœ¯æ–‡æ¡£": ["æŠ€æœ¯", "å¼€å‘", "ä»£ç ", "API", "æ¶æ„", "è®¾è®¡"],
            "å·¥ä½œæ€»ç»“": ["æ€»ç»“", "æ±‡æŠ¥", "æŠ¥å‘Š", "review", "summary"],
            "å­¦ä¹ ç¬”è®°": ["å­¦ä¹ ", "ç¬”è®°", "note", "æ•™ç¨‹", "è¯¾ç¨‹", "åŸ¹è®­"],
            "é‡è¦æ–‡æ¡£": ["é‡è¦", "å…³é”®", "æ ¸å¿ƒ", "urgent", "important"],
            "ç”Ÿæ´»è®°å½•": ["ç”Ÿæ´»", "æ—¥å¸¸", "ä¸ªäºº", "å®¶åº­", "æœ‹å‹", "ä¼‘é—²", "å¨±ä¹"],
            "ç¾é£Ÿ": ["ç¾é£Ÿ", "é¤å…", "èœè°±", "cooking", "food", "åƒé¥­", "æ–™ç†"],
            "å¥åº·": ["å¥åº·", "åŒ»ç–—", "è¿åŠ¨", "fitness", "health", "é”»ç‚¼", "ä½“æ£€", "å…»ç”Ÿ"],
            "è´¢åŠ¡": ["è´¢åŠ¡", "ç†è´¢", "æŠ•èµ„", "finance", "money", "é¢„ç®—", "è´¦å•", "æ”¶æ”¯"],
            "å®¶åº­": ["å®¶åº­", "family", "å­©å­", "çˆ¶æ¯", "äº²å­", "è‚²å„¿", "å®¶åŠ¡"],
            "å¨±ä¹": ["å¨±ä¹", "æ¸¸æˆ", "ç”µå½±", "éŸ³ä¹", "entertainment", "movie", "game", "music"],
            "è´­ç‰©": ["è´­ç‰©", "shopping", "å•†å“", "ä»·æ ¼", "ä¼˜æƒ ", "æŠ˜æ‰£", "å•†åŸ"],
            "æ•™è‚²": ["æ•™è‚²", "education", "åŸ¹è®­", "è¯¾ç¨‹", "å­¦æ ¡", "è€å¸ˆ", "å­¦ç”Ÿ"]
        }
        
        # ç›´æ¥åŒ¹é…
        if name in keyword_mapping:
            keywords.extend(keyword_mapping[name])
        
        # æ¨¡ç³ŠåŒ¹é…
        for key, values in keyword_mapping.items():
            if key in name or name in key:
                keywords.extend(values)
        
        # æ™ºèƒ½å…³é”®è¯æ‰©å±•ï¼šåŸºäºåˆé›†åç§°çš„è¯­ä¹‰åˆ†æ
        keywords.extend(self._generate_semantic_keywords(name))
        
        # æ·»åŠ åŸå§‹åç§°ä½œä¸ºå…³é”®è¯
        keywords.append(name)
        
        return list(set(keywords))  # å»é‡
    
    def _generate_semantic_keywords(self, name: str) -> List[str]:
        """åŸºäºåˆé›†åç§°ç”Ÿæˆè¯­ä¹‰ç›¸å…³çš„å…³é”®è¯"""
        semantic_keywords = []
        
        # åˆ†è¯å¤„ç†ï¼ˆç®€å•çš„ä¸­æ–‡åˆ†è¯é€»è¾‘ï¼‰
        import re
        # æå–ä¸­æ–‡è¯æ±‡
        chinese_words = re.findall(r'[\u4e00-\u9fff]+', name)
        for word in chinese_words:
            if len(word) >= 2:  # åªä¿ç•™é•¿åº¦>=2çš„è¯
                semantic_keywords.append(word)
        
        # æå–è‹±æ–‡è¯æ±‡
        english_words = re.findall(r'[a-zA-Z]+', name)
        for word in english_words:
            if len(word) >= 3:  # åªä¿ç•™é•¿åº¦>=3çš„è‹±æ–‡è¯
                semantic_keywords.append(word.lower())
        
        # åŸºäºå¸¸è§è¯æ±‡æ¨¡å¼æ‰©å±•
        name_lower = name.lower()
        
        # å¦‚æœåŒ…å«æ—¶é—´ç›¸å…³è¯æ±‡
        if any(time_word in name_lower for time_word in ['æ—¥è®°', 'è®°å½•', 'æ—¥å¿—', 'diary', 'log']):
            semantic_keywords.extend(['è®°å½•', 'æ—¥å¿—', 'ç¬”è®°'])
        
        # å¦‚æœåŒ…å«å·¥ä½œç›¸å…³è¯æ±‡
        if any(work_word in name_lower for work_word in ['å·¥ä½œ', 'work', 'èŒåœº', 'å…¬å¸']):
            semantic_keywords.extend(['å·¥ä½œ', 'èŒåœº', 'åŠå…¬'])
        
        # å¦‚æœåŒ…å«å­¦ä¹ ç›¸å…³è¯æ±‡
        if any(study_word in name_lower for study_word in ['å­¦ä¹ ', 'study', 'è¯¾ç¨‹', 'æ•™ç¨‹']):
            semantic_keywords.extend(['å­¦ä¹ ', 'æ•™è‚²', 'çŸ¥è¯†'])
        
        return semantic_keywords
    
    def _extract_keywords_from_description(self, description: str) -> List[str]:
        """ä»æè¿°ä¸­æå–å…³é”®è¯"""
        keywords = []
        
        # ç®€å•çš„å…³é”®è¯æå–ï¼ˆå¯ä»¥åç»­ç”¨NLPä¼˜åŒ–ï¼‰
        common_keywords = [
            "ä¼šè®®", "é¡¹ç›®", "å·¥ä½œ", "æŠ€æœ¯", "å­¦ä¹ ", "é‡è¦", "æ–‡æ¡£", "èµ„æ–™",
            "meeting", "project", "work", "tech", "study", "important", "document"
        ]
        
        desc_lower = description.lower()
        for keyword in common_keywords:
            if keyword in desc_lower:
                keywords.append(keyword)
        
        return keywords
    
    def _generate_title_patterns(self, collection_name: str, keywords: List[str]) -> List[str]:
        """ç”Ÿæˆæ ‡é¢˜åŒ¹é…æ¨¡å¼"""
        patterns = []
        
        # åŸºäºåˆé›†åç§°çš„æ¨¡å¼
        patterns.append(f".*{re.escape(collection_name)}.*")
        
        # åŸºäºå…³é”®è¯çš„æ¨¡å¼
        for keyword in keywords[:5]:  # åªå–å‰5ä¸ªå…³é”®è¯é¿å…è¿‡å¤šæ¨¡å¼
            patterns.append(f".*{re.escape(keyword)}.*")
        
        return patterns
    
    def _generate_content_patterns(self, keywords: List[str]) -> List[str]:
        """ç”Ÿæˆå†…å®¹åŒ¹é…æ¨¡å¼"""
        patterns = []
        
        # ä¼šè®®çºªè¦ç‰¹å®šæ¨¡å¼
        if any(k in keywords for k in ["ä¼šè®®", "meeting", "çºªè¦", "minutes"]):
            patterns.extend([
                "ä¼šè®®æ—¶é—´", "å‚ä¼šäººå‘˜", "ä¼šè®®è®®é¢˜", "ä¼šè®®å†…å®¹", "å†³è®®äº‹é¡¹",
                "meeting time", "attendees", "agenda", "action items"
            ])
        
        # é¡¹ç›®æ–‡æ¡£ç‰¹å®šæ¨¡å¼
        if any(k in keywords for k in ["é¡¹ç›®", "project"]):
            patterns.extend([
                "é¡¹ç›®èƒŒæ™¯", "é¡¹ç›®ç›®æ ‡", "é‡Œç¨‹ç¢‘", "deliverable", "timeline"
            ])
        
        return patterns
    
    def match_document_to_collections(self, content_id: str) -> List[str]:
        """
        å°†æ–‡æ¡£åŒ¹é…åˆ°åˆé€‚çš„ç”¨æˆ·åˆé›†
        
        Args:
            content_id: æ–‡æ¡£ID
            
        Returns:
            åŒ¹é…çš„åˆé›†IDåˆ—è¡¨
        """
        try:
            from uuid import UUID
            content_uuid = UUID(content_id) if isinstance(content_id, str) else content_id
            
            # è·å–æ–‡æ¡£å†…å®¹
            content = self.db.query(Content).filter(Content.id == content_uuid).first()
            if not content:
                logger.warning(f"Content not found: {content_id}")
                return []
            
            # æ£€æŸ¥å†…å®¹æ˜¯å¦æœ‰è¶³å¤Ÿçš„ä¿¡æ¯è¿›è¡ŒåŒ¹é…
            if not content.title and not content.text:
                logger.info(f"Content {content_id} has no title or text, skipping collection matching")
                return []
            
            # è·å–æ‰€æœ‰ç”¨æˆ·åˆ›å»ºçš„åˆé›†ï¼ˆåŒ…æ‹¬æ²¡æœ‰åŒ¹é…è§„åˆ™çš„ï¼‰
            collections = self.db.query(Collection).filter(
                Collection.auto_generated == False  # ç”¨æˆ·åˆ›å»ºçš„åˆé›†
            ).all()
            
            matched_collections = []
            
            logger.info(f"Found {len(collections)} user collections to check for content {content_id}")
            
            for collection in collections:
                logger.info(f"ğŸ” Checking collection '{collection.name}' (id: {collection.id}) for content '{content.title}'")
                
                # æ£€æŸ¥åˆé›†æ˜¯å¦æœ‰å…³è”çš„Category
                if not collection.category_id:
                    logger.warning(f"âš ï¸ Collection '{collection.name}' has no associated category_id, skipping")
                    continue
                
                if self._is_document_match_collection(content, collection):
                    # åˆ›å»ºæ–‡æ¡£-åˆé›†å…³è”
                    self._create_content_collection_association(content_id, str(collection.id))
                    matched_collections.append(str(collection.id))
                    logger.info(f"âœ… Document '{content.title}' matched to collection '{collection.name}'")
                else:
                    logger.info(f"âŒ Document '{content.title}' did not match collection '{collection.name}'")
            
            # ğŸ”¥ ä¿®å¤ï¼šä¸è¦è¦†ç›–AIåˆ†ç±»çš„çŠ¶æ€ï¼Œåªæ ‡è®°åˆé›†åŒ¹é…å®Œæˆ
            if content.meta is None:
                content.meta = {}
            
            # æ·»åŠ åˆé›†åŒ¹é…å®Œæˆæ ‡è®°ï¼Œä½†ä¸è¦†ç›–classification_status
            content.meta["collection_matching_status"] = "completed"
            content.meta["collection_matching_count"] = len(matched_collections)
            # ä¿æŒshow_classificationçŠ¶æ€ä¸å˜ï¼Œç”±AIåˆ†ç±»å†³å®š
            
            # æ ‡è®°metaå­—æ®µä¸ºå·²ä¿®æ”¹ï¼Œç¡®ä¿SQLAlchemyä¿å­˜æ›´æ”¹
            from sqlalchemy.orm.attributes import flag_modified
            flag_modified(content, 'meta')
            
            # æäº¤æ‰€æœ‰å…³è”åˆ›å»ºå’ŒçŠ¶æ€æ›´æ–°
            self.db.commit()
            
            if matched_collections:
                logger.info(f"Committed {len(matched_collections)} collection associations and updated status")
            else:
                logger.info(f"No collections matched, but updated status to completed")
            
            return matched_collections
            
        except Exception as e:
            logger.error(f"Error matching document to collections: {e}")
            self.db.rollback()
            return []
    
    def _is_document_match_collection(self, content: Content, collection: Collection) -> bool:
        """åˆ¤æ–­æ–‡æ¡£æ˜¯å¦åŒ¹é…åˆé›†"""
        try:
            rules = collection.query_rules
            
            # å¦‚æœæ²¡æœ‰åŒ¹é…è§„åˆ™ï¼ŒåŠ¨æ€ç”Ÿæˆï¼ˆä½†ä¸ç«‹å³ä¿å­˜ï¼Œé¿å…åœ¨åŒ¹é…è¿‡ç¨‹ä¸­æäº¤äº‹åŠ¡ï¼‰
            if not rules:
                rules = self.generate_auto_match_rules(collection.name, collection.description)
                if rules and rules.get("auto_match", False):
                    # ä¸´æ—¶è®¾ç½®è§„åˆ™ï¼Œä½†ä¸æäº¤åˆ°æ•°æ®åº“ï¼ˆé¿å…äº‹åŠ¡å†²çªï¼‰
                    collection.query_rules = rules
                    logger.info(f"Generated temporary rules for collection {collection.name}")
            
            if not rules or not rules.get("auto_match", False):
                return False
            
            match_score = 0
            threshold = rules.get("match_threshold", 0.3)  # é™ä½é˜ˆå€¼ä»¥ä¾¿è°ƒè¯•
            
            # æ ‡é¢˜åŒ¹é…
            title_score = self._calculate_title_match_score(content.title, rules)
            match_score += title_score * 0.3  # æ ‡é¢˜æƒé‡30%
            
            # å†…å®¹åŒ¹é…
            content_score = self._calculate_content_match_score(content.text, rules)
            match_score += content_score * 0.4  # å†…å®¹æƒé‡40%
            
            # å›¾ç‰‡æ´»åŠ¨æ¨ç†åŒ¹é…ï¼ˆæ–°å¢ï¼‰
            activity_score = self._calculate_activity_match_score(content, rules)
            match_score += activity_score * 0.3  # æ´»åŠ¨æ¨ç†æƒé‡30%
            
            logger.info(f"ğŸ¯ Match score for '{content.title}' -> '{collection.name}': "
                       f"title={title_score:.2f}, content={content_score:.2f}, activity={activity_score:.2f}, "
                       f"total={match_score:.2f}, threshold={threshold:.2f}, "
                       f"match={match_score >= threshold}")
            
            # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœæ˜¯æ˜æ˜¾çš„åŒ¹é…ï¼ˆå¦‚"è¿ªå£«å°¼"å›¾ç‰‡åŒ¹é…"æ—…æ¸¸"åˆé›†ï¼‰ï¼Œé™ä½é˜ˆå€¼
            if self._is_obvious_match(content, collection):
                logger.info(f"ğŸ¯ Obvious match detected, using lower threshold")
                return match_score >= 0.3  # é™ä½é˜ˆå€¼
            
            return match_score >= threshold
            
        except Exception as e:
            logger.error(f"Error calculating match score: {e}")
            return False
    
    def _is_obvious_match(self, content: Content, collection: Collection) -> bool:
        """æ£€æµ‹æ˜æ˜¾çš„åŒ¹é…æƒ…å†µ"""
        try:
            title = content.title.lower() if content.title else ""
            collection_name = collection.name.lower()
            
            # æ˜æ˜¾åŒ¹é…çš„æ¨¡å¼
            obvious_patterns = {
                "æ—…æ¸¸": ["è¿ªå£«å°¼", "æ™¯åŒº", "æ™¯ç‚¹", "é£æ™¯", "æ—…è¡Œ", "åº¦å‡", "é…’åº—", "å¥—é¤"],
                "ä¼šè®®çºªè¦": ["ä¼šè®®", "çºªè¦", "meeting", "minutes"],
                "å·¥ä½œ": ["å·¥ä½œ", "é¡¹ç›®", "æŠ¥å‘Š", "æ€»ç»“"],
                "å­¦ä¹ ": ["å­¦ä¹ ", "ç¬”è®°", "æ•™ç¨‹", "è¯¾ç¨‹"]
            }
            
            for pattern_key, keywords in obvious_patterns.items():
                if pattern_key in collection_name:
                    for keyword in keywords:
                        if keyword in title:
                            logger.info(f"ğŸ¯ Obvious match: '{title}' contains '{keyword}' for collection '{collection_name}'")
                            return True
            
            return False
            
        except Exception as e:
            logger.error(f"Error in _is_obvious_match: {e}")
            return False
    
    def _calculate_title_match_score(self, title: str, rules: Dict) -> float:
        """è®¡ç®—æ ‡é¢˜åŒ¹é…åˆ†æ•°"""
        if not title:
            return 0.0
        
        title_lower = title.lower()
        score = 0.0
        
        # å…³é”®è¯åŒ¹é…ï¼ˆä¼˜åŒ–ï¼šä½¿ç”¨åŒ¹é…æ•°é‡è€Œä¸æ˜¯æ¯”ä¾‹ï¼Œé¿å…å…³é”®è¯è¿‡å¤šç¨€é‡Šåˆ†æ•°ï¼‰
        keywords = rules.get("keywords", [])
        if keywords:
            matched_keywords = sum(1 for keyword in keywords if keyword.lower() in title_lower)
            # ä½¿ç”¨åŒ¹é…å…³é”®è¯æ•°é‡ï¼Œæ¯ä¸ªåŒ¹é…çš„å…³é”®è¯è´¡çŒ®0.2åˆ†ï¼Œæœ€å¤§0.7åˆ†
            score += min(0.7, matched_keywords * 0.2)
        
        # æ¨¡å¼åŒ¹é…
        patterns = rules.get("title_patterns", [])
        if patterns:
            matched_patterns = sum(1 for pattern in patterns if re.search(pattern, title, re.IGNORECASE))
            score += (matched_patterns / len(patterns)) * 0.3
        
        return min(1.0, score)
    
    def _calculate_content_match_score(self, text: str, rules: Dict) -> float:
        """è®¡ç®—å†…å®¹åŒ¹é…åˆ†æ•°"""
        if not text:
            return 0.0
        
        # åªåˆ†æå‰1000å­—ç¬¦ä»¥æé«˜æ€§èƒ½
        text_sample = text[:1000].lower()
        score = 0.0
        
        # å…³é”®è¯åŒ¹é…ï¼ˆä¼˜åŒ–ï¼šä½¿ç”¨åŒ¹é…æ•°é‡ï¼‰
        keywords = rules.get("keywords", [])
        if keywords:
            matched_keywords = sum(1 for keyword in keywords if keyword.lower() in text_sample)
            # æ¯ä¸ªåŒ¹é…çš„å…³é”®è¯è´¡çŒ®0.15åˆ†ï¼Œæœ€å¤§0.6åˆ†
            score += min(0.6, matched_keywords * 0.15)
        
        # å†…å®¹æ¨¡å¼åŒ¹é…
        content_patterns = rules.get("content_patterns", [])
        if content_patterns:
            matched_patterns = sum(1 for pattern in content_patterns if pattern.lower() in text_sample)
            score += (matched_patterns / len(content_patterns)) * 0.4
        
        return min(1.0, score)
    
    def match_existing_documents_to_collection(self, collection_id: str) -> int:
        """
        å°†ç°æœ‰æ–‡æ¡£åŒ¹é…åˆ°æ–°åˆ›å»ºçš„åˆé›†
        
        Args:
            collection_id: åˆé›†ID
            
        Returns:
            åŒ¹é…çš„æ–‡æ¡£æ•°é‡
        """
        try:
            from uuid import UUID
            collection_uuid = UUID(collection_id) if isinstance(collection_id, str) else collection_id
            
            collection = self.db.query(Collection).filter(Collection.id == collection_uuid).first()
            if not collection or not collection.query_rules:
                return 0
            
            # è·å–æ‰€æœ‰æ–‡æ¡£
            contents = self.db.query(Content).all()
            matched_count = 0
            
            for content in contents:
                if self._is_document_match_collection(content, collection):
                    # åˆ›å»ºæ–‡æ¡£-åˆé›†å…³è”
                    self._create_content_collection_association(str(content.id), collection_id)
                    matched_count += 1
            
            logger.info(f"Matched {matched_count} existing documents to collection {collection.name}")
            return matched_count
            
        except Exception as e:
            logger.error(f"Error matching existing documents to collection: {e}")
            return 0
    
    def _create_content_collection_association(self, content_id: str, collection_id: str):
        """åˆ›å»ºæ–‡æ¡£-åˆé›†å…³è”"""
        try:
            from uuid import UUID
            
            content_uuid = UUID(content_id)
            collection_uuid = UUID(collection_id)
            
            # è·å–åˆé›†å¯¹åº”çš„åˆ†ç±»ID
            collection = self.db.query(Collection).filter(Collection.id == collection_uuid).first()
            if not collection or not collection.category_id:
                logger.warning(f"Collection {collection_id} has no associated category")
                return
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒreasoningçš„å…³è”ï¼ˆé¿å…é‡å¤ï¼Œä½†å…è®¸ä¸åŒæ¥æºçš„åˆ†ç±»ï¼‰
            existing = self.db.query(ContentCategory).filter(
                ContentCategory.content_id == content_uuid,
                ContentCategory.category_id == collection.category_id,
                ContentCategory.reasoning.like(f"%è‡ªåŠ¨åŒ¹é…åˆ°åˆé›†: {collection.name}%")
            ).first()
            
            if existing:
                logger.debug(f"Collection association already exists: content {content_id} -> collection {collection.name}")
                return
            
            # åˆ›å»ºæ–°çš„å…³è”
            content_category = ContentCategory(
                content_id=content_uuid,
                category_id=collection.category_id,
                confidence=0.8,  # è‡ªåŠ¨åŒ¹é…çš„ç½®ä¿¡åº¦
                reasoning=f"è‡ªåŠ¨åŒ¹é…åˆ°åˆé›†: {collection.name}",
                role="user_rule",  # ç”¨æˆ·è§„åˆ™åˆ†ç±»
                source="rule"      # åŸºäºè§„åˆ™çš„åˆ†ç±»
            )
            
            self.db.add(content_category)
            # ä¸åœ¨è¿™é‡Œæäº¤ï¼Œè®©è°ƒç”¨è€…å†³å®šä½•æ—¶æäº¤äº‹åŠ¡
            self.db.flush()  # ç¡®ä¿å¯¹è±¡è¢«æŒä¹…åŒ–åˆ°ä¼šè¯ä¸­
            
            logger.info(f"Created association: content {content_id} -> collection {collection_id}")
            
        except Exception as e:
            logger.error(f"Error creating content-collection association: {e}")
            self.db.rollback()
    
    def _calculate_activity_match_score(self, content: Content, rules: Dict) -> float:
        """åŸºäºå›¾ç‰‡æ´»åŠ¨æ¨ç†è®¡ç®—åŒ¹é…åˆ†æ•°"""
        try:
            # å¦‚æœä¸æ˜¯å›¾ç‰‡æˆ–æ²¡æœ‰æ–‡æœ¬å†…å®¹ï¼Œè¿”å›0
            if content.modality != 'image' or not content.text:
                return 0.0
            
            # è§£æå›¾ç‰‡åˆ†æç»“æœ
            activity_info = self._parse_image_analysis(content.text)
            if not activity_info:
                return 0.0
            
            # è·å–åˆé›†å…³é”®è¯
            keywords = rules.get("keywords", [])
            if not keywords:
                return 0.0
            
            score = 0.0
            total_checks = 0
            
            # æ£€æŸ¥æ´»åŠ¨æ¨ç†åŒ¹é…
            activity_inference = activity_info.get("activity_inference", "").lower()
            if activity_inference:
                total_checks += 1
                for keyword in keywords:
                    if keyword.lower() in activity_inference:
                        score += 0.8  # æ´»åŠ¨æ¨ç†åŒ¹é…æƒé‡é«˜
                        logger.debug(f"Activity inference match: '{keyword}' in '{activity_inference}'")
                        break
            
            # æ£€æŸ¥å…³é”®å…ƒç´ åŒ¹é…
            key_elements = activity_info.get("key_elements", "").lower()
            if key_elements:
                total_checks += 1
                keyword_matches = 0
                for keyword in keywords:
                    if keyword.lower() in key_elements:
                        keyword_matches += 1
                        logger.debug(f"Key element match: '{keyword}' in '{key_elements}'")
                
                if keyword_matches > 0:
                    score += (keyword_matches / len(keywords)) * 0.6  # å…³é”®å…ƒç´ åŒ¹é…
            
            # æ£€æŸ¥åœºæ™¯æè¿°åŒ¹é…
            scene_description = activity_info.get("scene_description", "").lower()
            if scene_description:
                total_checks += 1
                for keyword in keywords:
                    if keyword.lower() in scene_description:
                        score += 0.4  # åœºæ™¯æè¿°åŒ¹é…æƒé‡è¾ƒä½
                        logger.debug(f"Scene description match: '{keyword}' in '{scene_description}'")
                        break
            
            # å½’ä¸€åŒ–åˆ†æ•°
            final_score = min(score, 1.0) if total_checks > 0 else 0.0
            
            logger.debug(f"Activity match score: {final_score:.2f} for content '{content.title}'")
            return final_score
            
        except Exception as e:
            logger.error(f"Error calculating activity match score: {e}")
            return 0.0
    
    def _parse_image_analysis(self, text: str) -> Dict[str, str]:
        """è§£æå›¾ç‰‡åˆ†æç»“æœï¼Œæå–ç»“æ„åŒ–ä¿¡æ¯"""
        try:
            if not text:
                return {}
            
            result = {}
            
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å„ä¸ªéƒ¨åˆ†
            import re
            
            # æå–æ´»åŠ¨æ¨ç†
            activity_match = re.search(r'ã€æ´»åŠ¨æ¨ç†ã€‘\s*(.*?)(?=ã€|$)', text, re.DOTALL)
            if activity_match:
                result["activity_inference"] = activity_match.group(1).strip()
            
            # æå–å…³é”®å…ƒç´ 
            elements_match = re.search(r'ã€å…³é”®å…ƒç´ ã€‘\s*(.*?)(?=ã€|$)', text, re.DOTALL)
            if elements_match:
                result["key_elements"] = elements_match.group(1).strip()
            
            # æå–åœºæ™¯æè¿°
            scene_match = re.search(r'ã€åœºæ™¯æè¿°ã€‘\s*(.*?)(?=ã€|$)', text, re.DOTALL)
            if scene_match:
                result["scene_description"] = scene_match.group(1).strip()
            
            # æå–åˆ†ç±»å»ºè®®
            classification_match = re.search(r'ã€åˆ†ç±»å»ºè®®ã€‘\s*(.*?)(?=ã€|$)', text, re.DOTALL)
            if classification_match:
                result["classification_suggestion"] = classification_match.group(1).strip()
            
            logger.debug(f"Parsed image analysis: {result}")
            return result
            
        except Exception as e:
            logger.error(f"Error parsing image analysis: {e}")
            return {}
