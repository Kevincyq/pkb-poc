from .celery_app import celery_app
from app.db import SessionLocal
from app.models import Chunk, Content
from app.services.embedding_service import EmbeddingService
from app.services.category_service import CategoryService
from app.parsers.document_processor import DocumentProcessor
import logging
import os
from pathlib import Path
from datetime import datetime

logger = logging.getLogger(__name__)

# æ–‡æ¡£åˆ†å—å‡½æ•°
def simple_chunk(text: str, max_len: int = 700):
    """
    ç®€å•çš„æ–‡æœ¬åˆ†å—å‡½æ•°
    
    Args:
        text: è¦åˆ†å—çš„æ–‡æœ¬
        max_len: æ¯å—æœ€å¤§é•¿åº¦
        
    Returns:
        æ–‡æœ¬å—åˆ—è¡¨
    """
    if not text:
        return []
    
    buf, chunks = [], []
    for line in text.splitlines():
        if len("".join(buf)) + len(line) > max_len:
            if buf:
                chunks.append("\n".join(buf))
                buf = []
        buf.append(line)
    
    if buf:
        chunks.append("\n".join(buf))
    
    return [c.strip() for c in chunks if c.strip()]

# æ–°å¢ï¼šæ–‡ä»¶è§£æå’Œåˆ†å—ä»»åŠ¡
@celery_app.task(name="app.workers.tasks.parse_and_chunk_file", queue="quick")
def parse_and_chunk_file(content_id: str, file_path: str):
    """
    å¼‚æ­¥è§£ææ–‡ä»¶å†…å®¹å¹¶è¿›è¡Œåˆ†å—
    
    Args:
        content_id: å†…å®¹ID
        file_path: æ–‡ä»¶è·¯å¾„
    """
    db = SessionLocal()
    try:
        logger.info(f"ğŸ” Starting file parsing for content {content_id}: {file_path}")
        
        # è·å–å†…å®¹è®°å½•
        content = db.query(Content).filter(Content.id == content_id).first()
        if not content:
            logger.error(f"Content {content_id} not found")
            return {"status": "error", "message": "Content not found"}
        
        # æ›´æ–°çŠ¶æ€ä¸ºè§£æä¸­
        if content.meta:
            content.meta["parsing_status"] = "parsing"
            content.meta["processing_status"] = "parsing"
            # æ ‡è®°metaå­—æ®µä¸ºå·²ä¿®æ”¹
            from sqlalchemy.orm.attributes import flag_modified
            flag_modified(content, 'meta')
        db.commit()
        
        # è§£ææ–‡ä»¶å†…å®¹
        processor = DocumentProcessor()
        try:
            parsed_result = processor.process_file(file_path)
            file_text = parsed_result.get("text", "")
            file_metadata = parsed_result.get("metadata", {})
            
            logger.info(f"ğŸ“„ Parsed file {content_id}: {len(file_text)} chars, metadata: {list(file_metadata.keys())}")
            
        except Exception as e:
            logger.error(f"Failed to parse file {content_id}: {e}")
            file_text = ""
            file_metadata = {"parse_error": str(e)}
        
        # æ›´æ–°å†…å®¹è®°å½•
        content.text = file_text
        if content.meta:
            content.meta.update(file_metadata)
            content.meta["parsing_status"] = "completed"
            content.meta["processing_status"] = "parsed"
            # æ ‡è®°metaå­—æ®µä¸ºå·²ä¿®æ”¹
            from sqlalchemy.orm.attributes import flag_modified
            flag_modified(content, 'meta')
        
        # è¿›è¡Œæ–‡æœ¬åˆ†å—
        chunk_ids = []
        if file_text:
            seq = 0
            for chunk_text in simple_chunk(file_text):
                chunk = Chunk(
                    content_id=content.id,
                    seq=seq,
                    text=chunk_text,
                    meta={"source_uri": content.source_uri}
                )
                db.add(chunk)
                seq += 1
            
            db.commit()
            db.refresh(content)
            chunk_ids = [str(chunk.id) for chunk in content.chunks]
            
            logger.info(f"ğŸ“ Created {len(chunk_ids)} chunks for content {content_id}")
            
            # ç«‹å³è°ƒåº¦å‘é‡ç”Ÿæˆä»»åŠ¡
            generate_embeddings.apply_async(
                args=[chunk_ids],
                queue="heavy",
                priority=8,
                countdown=0.5
            )
        
        db.commit()
        
        return {
            "status": "success",
            "content_id": content_id,
            "chunks_created": len(chunk_ids),
            "text_length": len(file_text),
            "message": "æ–‡ä»¶è§£æå®Œæˆ"
        }
        
    except Exception as e:
        logger.error(f"Parse and chunk task failed for {content_id}: {e}")
        
        # æ›´æ–°é”™è¯¯çŠ¶æ€
        try:
            content = db.query(Content).filter(Content.id == content_id).first()
            if content and content.meta:
                content.meta["parsing_status"] = "error"
                content.meta["processing_status"] = "error"
                content.meta["parse_error"] = str(e)
            db.commit()
        except Exception as db_e:
            logger.error(f"Failed to update error status: {db_e}")
        
        return {"status": "error", "message": str(e)}
    finally:
        db.close()

# æ–°å¢ï¼šå›¾ç‰‡ç¼©ç•¥å›¾ç”Ÿæˆä»»åŠ¡
@celery_app.task(name="app.workers.tasks.generate_image_thumbnail", queue="heavy")
def generate_image_thumbnail(content_id: str, file_path: str):
    """
    å¼‚æ­¥ç”Ÿæˆå›¾ç‰‡ç¼©ç•¥å›¾
    
    Args:
        content_id: å†…å®¹ID
        file_path: å›¾ç‰‡æ–‡ä»¶è·¯å¾„
    """
    try:
        logger.info(f"ğŸ–¼ï¸  Generating thumbnail for content {content_id}: {file_path}")
        
        from app.api.files import pregenerate_thumbnail_if_image
        if pregenerate_thumbnail_if_image(Path(file_path)):
            logger.info(f"âœ… Thumbnail generated for content {content_id}")
            return {"status": "success", "message": "Thumbnail generated"}
        else:
            logger.warning(f"âš ï¸  No thumbnail generated for content {content_id}")
            return {"status": "skipped", "message": "Not an image or thumbnail exists"}
            
    except Exception as e:
        logger.error(f"âŒ Thumbnail generation failed for {content_id}: {e}")
        return {"status": "error", "message": str(e)}

# å…¥å£ä»»åŠ¡ï¼šå¤„ç†æ–‡ä»¶æ‘„å–
@celery_app.task(name="app.workers.tasks.ingest_file", queue="ingest")
def ingest_file(path: str):
    """
    å¤„ç†æ–‡ä»¶æ‘„å–ä»»åŠ¡
    æ”¯æŒæœ¬åœ°æ–‡ä»¶å’Œ WebDAV æ–‡ä»¶çš„è§£æå’Œå…¥åº“
    
    Args:
        path: æ–‡ä»¶è·¯å¾„ï¼ˆæœ¬åœ°è·¯å¾„æˆ– WebDAV URLï¼‰
        
    Returns:
        å¤„ç†ç»“æœå­—å…¸
    """
    db = SessionLocal()
    try:
        logger.info(f"Starting file ingestion for: {path}")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼ˆæœ¬åœ°æ–‡ä»¶ï¼‰
        if not path.startswith(('http://', 'https://')) and not os.path.exists(path):
            error_msg = f"File not found: {path}"
            logger.error(error_msg)
            return {"ok": False, "error": error_msg}
        
        # åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨
        processor = DocumentProcessor()
        
        # å¤„ç†æ–‡ä»¶
        if path.startswith(('http://', 'https://')):
            # WebDAV æ–‡ä»¶å¤„ç†
            filename = Path(path).name
            from app.adapters.webdav import download_and_parse_file
            parse_result = download_and_parse_file(path, filename)
        else:
            # æœ¬åœ°æ–‡ä»¶å¤„ç†
            parse_result = processor.process_file(path)
        
        # æ£€æŸ¥è§£æç»“æœ
        if not parse_result.get('success', True):
            error_msg = f"Failed to parse file: {parse_result.get('metadata', {}).get('error', 'Unknown error')}"
            logger.error(error_msg)
            return {"ok": False, "error": error_msg}
        
        text_content = parse_result.get('text', '')
        if not text_content.strip():
            error_msg = f"No text content extracted from file: {path}"
            logger.warning(error_msg)
            return {"ok": False, "error": error_msg}
        
        # åˆ›å»º Content è®°å½•
        filename = parse_result.get('metadata', {}).get('filename', Path(path).name)
        title = parse_result.get('metadata', {}).get('title', filename)
        
        # ç¡®å®šæ–‡æ¡£ç±»å‹
        detected_type = parse_result.get('metadata', {}).get('detected_type', 'text')
        modality = 'image' if detected_type == 'image' else 'text'
        
        content = Content(
            title=title,
            text=text_content,
            modality=modality,
            meta=parse_result.get('metadata', {}),
            source_uri=f"file://{path}",
            created_by="celery.ingest"
        )
        
        db.add(content)
        db.commit()
        db.refresh(content)
        
        # æ–‡æœ¬åˆ†å—
        chunks = simple_chunk(text_content)
        chunk_ids = []
        
        for seq, chunk_text in enumerate(chunks):
            chunk = Chunk(
                content_id=content.id,
                seq=seq,
                text=chunk_text,
                meta={
                    "source_uri": content.source_uri,
                    "chunk_index": seq,
                    "total_chunks": len(chunks)
                }
            )
            db.add(chunk)
            chunk_ids.append(chunk.id)
        
        db.commit()
        
        # ç«‹å³è¿›è¡Œå¿«é€Ÿåˆ†ç±»ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰
        from app.workers.quick_tasks import quick_classify_content
        quick_classify_content.apply_async(
            args=[str(content.id)], 
            queue="quick", 
            priority=9
        )
        
        # å¼‚æ­¥ç”Ÿæˆ embeddings
        if chunk_ids:
            chunk_ids_str = [str(cid) for cid in chunk_ids]
            generate_embeddings.delay(chunk_ids_str)
        
        # å¼‚æ­¥è¿›è¡Œç²¾ç¡®AIåˆ†ç±»ï¼ˆè¾ƒä½ä¼˜å…ˆçº§ï¼Œä¼šè¦†ç›–å¿«é€Ÿåˆ†ç±»ï¼‰
        classify_content.apply_async(
            args=[str(content.id)], 
            queue="classify", 
            priority=5,
            countdown=30  # å»¶è¿Ÿ30ç§’æ‰§è¡Œï¼Œè®©ç”¨æˆ·å…ˆçœ‹åˆ°å¿«é€Ÿåˆ†ç±»ç»“æœ
        )
        
        result = {
            "ok": True,
            "content_id": str(content.id),
            "chunks_created": len(chunks),
            "title": title,
            "text_length": len(text_content),
            "file_type": parse_result.get('metadata', {}).get('detected_type', 'unknown')
        }
        
        logger.info(f"Successfully ingested file {path}: {len(chunks)} chunks created")
        return result
        
    except Exception as e:
        logger.error(f"Error ingesting file {path}: {e}")
        db.rollback()
        return {"ok": False, "error": str(e)}
    finally:
        db.close()

# ç”Ÿæˆ embedding çš„åå°ä»»åŠ¡
@celery_app.task(name="app.workers.tasks.generate_embeddings", queue="heavy")
def generate_embeddings(chunk_ids: list):
    """
    ä¸ºæŒ‡å®šçš„ chunks ç”Ÿæˆ embedding
    """
    db = SessionLocal()
    embedding_service = EmbeddingService()
    
    try:
        if not embedding_service.is_enabled():
            logger.warning("Embedding service not available")
            return {"ok": False, "error": "Embedding service not configured"}
        
        # è·å–éœ€è¦å¤„ç†çš„ chunks
        chunks = db.query(Chunk).filter(Chunk.id.in_(chunk_ids)).all()
        
        if not chunks:
            return {"ok": True, "processed": 0}
        
        # æ‰¹é‡ç”Ÿæˆ embeddings
        texts = [chunk.text for chunk in chunks]
        embeddings = embedding_service.batch_get_embeddings(texts)
        
        # æ›´æ–°æ•°æ®åº“
        processed_count = 0
        for chunk, embedding in zip(chunks, embeddings):
            if embedding:
                chunk.embedding = embedding
                chunk.char_count = len(chunk.text)
                chunk.token_count = int(len(chunk.text.split()) * 1.3)  # ç®€å•ä¼°ç®—
                processed_count += 1
        
        db.commit()
        
        logger.info(f"Generated embeddings for {processed_count} chunks")
        return {"ok": True, "processed": processed_count}
        
    except Exception as e:
        logger.error(f"Error generating embeddings: {e}")
        db.rollback()
        return {"ok": False, "error": str(e)}
    finally:
        db.close()

# å¯ä»¥æ·»åŠ å…¶ä»–å¤„ç†ä»»åŠ¡ï¼Œå¦‚æ–‡æ¡£è§£æã€OCRç­‰
@celery_app.task(name="app.workers.tasks.process_document", queue="heavy")
def process_document(content_id: str, doc_type: str = "text"):
    """
    å¤„ç†æ–‡æ¡£çš„åå°ä»»åŠ¡ï¼Œå¦‚OCRã€æ ¼å¼è½¬æ¢ç­‰
    """
    logger.info(f"process_document called with content_id: {content_id}, type: {doc_type}")
    return {"ok": True, "content_id": content_id, "processed": True}

@celery_app.task(name="app.workers.tasks.classify_content", queue="classify")
def classify_content(content_id: str):
    """
    å¯¹å†…å®¹è¿›è¡Œæ™ºèƒ½åˆ†ç±»
    
    Args:
        content_id: å†…å®¹ID
        
    Returns:
        åˆ†ç±»ç»“æœ
    """
    db = SessionLocal()
    try:
        logger.info(f"Starting classification for content: {content_id}")
        
        # ğŸ”¥ ä¿®å¤ï¼šæ£€æŸ¥è§£æçŠ¶æ€ï¼Œå¦‚æœè¿˜åœ¨è§£æä¸­åˆ™å»¶è¿Ÿæ‰§è¡Œ
        content = db.query(Content).filter(Content.id == content_id).first()
        if content and content.meta:
            parsing_status = content.meta.get("parsing_status", "pending")
            if parsing_status == "parsing":
                logger.warning(f"â° Content {content_id} still parsing, retrying in 3 seconds")
                # å»¶è¿Ÿé‡è¯•
                classify_content.apply_async(
                    args=[content_id],
                    queue="classify",
                    priority=8,
                    countdown=3
                )
                return {"success": False, "error": "Still parsing, retrying"}
            
            # æ›´æ–°çŠ¶æ€ä¸ºAIåˆ†ç±»ä¸­
            content.meta["classification_status"] = "ai_processing"
            # æ ‡è®°metaå­—æ®µä¸ºå·²ä¿®æ”¹
            from sqlalchemy.orm.attributes import flag_modified
            flag_modified(content, 'meta')
        db.commit()
        
        # åˆå§‹åŒ–åˆ†ç±»æœåŠ¡
        category_service = CategoryService(db)
        
        # ç¡®ä¿ç³»ç»Ÿåˆ†ç±»å·²åˆå§‹åŒ–
        category_service.initialize_system_categories()
        
        # æ‰§è¡Œåˆ†ç±»
        result = category_service.classify_content(content_id)
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šæ— è®ºæˆåŠŸå¤±è´¥ï¼Œéƒ½è®¾ç½®show_classification = True
        if content and content.meta:
            if result["success"]:
                content.meta["classification_status"] = "completed"
                content.meta["show_classification"] = True  # å…è®¸å‰ç«¯æ˜¾ç¤ºç»“æœ
                logger.info(f"âœ… Successfully classified content {content_id} as {result.get('category_name', 'unknown')}")
            else:
                content.meta["classification_status"] = "error"
                content.meta["show_classification"] = True  # å³ä½¿å¤±è´¥ä¹Ÿæ˜¾ç¤ºçŠ¶æ€
                content.meta["classification_error"] = result.get('error', 'unknown error')
                logger.error(f"âŒ Failed to classify content {content_id}: {result.get('error', 'unknown error')}")
            # æ ‡è®°metaå­—æ®µä¸ºå·²ä¿®æ”¹
            from sqlalchemy.orm.attributes import flag_modified
            flag_modified(content, 'meta')
        db.commit()
        
        return result
        
    except Exception as e:
        logger.error(f"Error in classify_content task for {content_id}: {e}")
        return {"success": False, "error": str(e)}
    finally:
        db.close()

@celery_app.task(name="app.workers.tasks.batch_classify_contents", queue="classify")
def batch_classify_contents(content_ids: list, force_reclassify: bool = False):
    """
    æ‰¹é‡åˆ†ç±»å†…å®¹
    
    Args:
        content_ids: å†…å®¹IDåˆ—è¡¨
        force_reclassify: æ˜¯å¦å¼ºåˆ¶é‡æ–°åˆ†ç±»
        
    Returns:
        æ‰¹é‡åˆ†ç±»ç»“æœ
    """
    db = SessionLocal()
    try:
        logger.info(f"Starting batch classification for {len(content_ids)} contents")
        
        # åˆå§‹åŒ–åˆ†ç±»æœåŠ¡
        category_service = CategoryService(db)
        
        # ç¡®ä¿ç³»ç»Ÿåˆ†ç±»å·²åˆå§‹åŒ–
        category_service.initialize_system_categories()
        
        # æ‰§è¡Œæ‰¹é‡åˆ†ç±»
        result = category_service.batch_classify(content_ids, force_reclassify)
        
        logger.info(f"Batch classification completed: {result['success']} success, {result['failed']} failed")
        
        return result
        
    except Exception as e:
        logger.error(f"Error in batch_classify_contents task: {e}")
        return {"success": False, "error": str(e)}
    finally:
        db.close()

@celery_app.task(name="app.workers.tasks.process_image_content", queue="heavy")
def process_image_content(content_id: str):
    """
    å¼‚æ­¥å¤„ç†å›¾ç‰‡å†…å®¹
    
    Args:
        content_id: å†…å®¹ID
        
    Returns:
        å¤„ç†ç»“æœ
    """
    db = SessionLocal()
    try:
        logger.info(f"Starting image processing for content: {content_id}")
        
        # è·å–å†…å®¹è®°å½•
        content = db.query(Content).filter(Content.id == content_id).first()
        if not content:
            logger.error(f"Content not found: {content_id}")
            return {"success": False, "error": "Content not found"}
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å¤„ç†
        current_status = content.meta.get("processing_status")
        if current_status == "completed":
            logger.info(f"Content {content_id} already processed")
            return {"success": True, "status": "already_processed"}
        
        # æ›´æ–°ä¸ºå¤„ç†ä¸­çŠ¶æ€
        content.meta["processing_status"] = "processing"
        content.updated_at = datetime.utcnow()
        db.commit()
        
        # è·å–ä¸‹è½½URL
        download_url = content.meta.get("download_url")
        if not download_url:
            logger.error(f"No download URL for content {content_id}")
            return {"success": False, "error": "No download URL"}
        
        # ä¸‹è½½å¹¶è§£æå›¾ç‰‡
        logger.info(f"Processing image content: {content.title}")
        from app.adapters.webdav import download_and_parse_file
        parse_result = download_and_parse_file(download_url, content.title)
        
        if parse_result.get('success', True) and parse_result.get('text'):
            # æ›´æ–°å†…å®¹
            content.text = parse_result['text']
            content.meta.update(parse_result.get('metadata', {}))
            content.meta["processing_status"] = "completed"
            content.updated_at = datetime.utcnow()
            
            # é‡æ–°åˆ†å—
            # åˆ é™¤æ—§çš„chunks
            db.query(Chunk).filter(Chunk.content_id == content.id).delete()
            
            # åˆ›å»ºæ–°çš„chunks
            seq = 0
            chunk_ids = []
            for t in simple_chunk(parse_result['text']):
                chunk = Chunk(
                    content_id=content.id,
                    seq=seq,
                    text=t,
                    meta={"source_uri": content.source_uri}
                )
                db.add(chunk)
                seq += 1
            
            db.commit()
            
            # è·å–æ–°çš„chunk IDs
            db.refresh(content)
            chunk_ids = [str(chunk.id) for chunk in content.chunks]
            
            # å¼‚æ­¥ç”Ÿæˆembeddings
            if chunk_ids:
                generate_embeddings.delay(chunk_ids)
            
            # è§¦å‘åˆ†ç±»
            from app.workers.quick_tasks import quick_classify_content
            quick_classify_content.apply_async(
                args=[str(content.id)], 
                queue="quick", 
                priority=9
            )
            
            classify_content.apply_async(
                args=[str(content.id)], 
                queue="classify", 
                priority=5,
                countdown=30
            )
            
            logger.info(f"Successfully processed image content: {content.title}")
            return {"success": True, "chunks": seq}
        else:
            # å¤„ç†å¤±è´¥
            content.meta["processing_status"] = "failed"
            content.meta["error"] = parse_result.get('metadata', {}).get('error', 'Unknown error')
            db.commit()
            
            logger.error(f"Failed to process image content: {content.title}")
            return {"success": False, "error": content.meta.get("error")}
            
    except Exception as e:
        logger.error(f"Error in process_image_content task: {e}")
        return {"success": False, "error": str(e)}
    finally:
        db.close()
